{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import spacy\n",
    "from synth_data import *\n",
    "from helpers import *\n",
    "\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESL Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load in the ESL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "with open(\"lang-8_data.dat\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "            records.append(obj)\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the unstructured nature of the data, we will add column names and variables in order to work with the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for rec in records:\n",
    "    journal_id = rec[0]\n",
    "    sentence_id = rec[1]\n",
    "    learning_language = rec[2]\n",
    "    native_language = rec[3]\n",
    "    learner_sents = rec[4]\n",
    "    corrections = rec[5]\n",
    "\n",
    "    for sent, corr_list in zip(learner_sents, corrections):\n",
    "        rows.append({\n",
    "            \"journal_id\": journal_id,\n",
    "            \"sentence_id\": sentence_id,\n",
    "            \"learning_language\": learning_language,\n",
    "            \"native_language\": native_language,\n",
    "            \"sentence\": sent,\n",
    "            \"corrections\": corr_list\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will save this as a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(rows)\n\u001b[1;32m      3\u001b[0m df\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fin_proj/lib/python3.11/site-packages/pandas/core/frame.py:855\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[39mif\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    854\u001b[0m         columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 855\u001b[0m     arrays, columns, index \u001b[39m=\u001b[39m nested_data_to_arrays(\n\u001b[1;32m    856\u001b[0m         \u001b[39m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[1;32m    857\u001b[0m         \u001b[39m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[1;32m    858\u001b[0m         data,\n\u001b[1;32m    859\u001b[0m         columns,\n\u001b[1;32m    860\u001b[0m         index,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    861\u001b[0m         dtype,\n\u001b[1;32m    862\u001b[0m     )\n\u001b[1;32m    863\u001b[0m     mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    864\u001b[0m         arrays,\n\u001b[1;32m    865\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[1;32m    869\u001b[0m     )\n\u001b[1;32m    870\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fin_proj/lib/python3.11/site-packages/pandas/core/internals/construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[39mif\u001b[39;00m is_named_tuple(data[\u001b[39m0\u001b[39m]) \u001b[39mand\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     columns \u001b[39m=\u001b[39m ensure_index(data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_fields)\n\u001b[0;32m--> 520\u001b[0m arrays, columns \u001b[39m=\u001b[39m to_arrays(data, columns, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    521\u001b[0m columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fin_proj/lib/python3.11/site-packages/pandas/core/internals/construction.py:837\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    835\u001b[0m     arr \u001b[39m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m    836\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data[\u001b[39m0\u001b[39m], abc\u001b[39m.\u001b[39mMapping):\n\u001b[0;32m--> 837\u001b[0m     arr, columns \u001b[39m=\u001b[39m _list_of_dict_to_arrays(data, columns)\n\u001b[1;32m    838\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m    839\u001b[0m     arr, columns \u001b[39m=\u001b[39m _list_of_series_to_arrays(data, columns)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fin_proj/lib/python3.11/site-packages/pandas/core/internals/construction.py:917\u001b[0m, in \u001b[0;36m_list_of_dict_to_arrays\u001b[0;34m(data, columns)\u001b[0m\n\u001b[1;32m    915\u001b[0m     gen \u001b[39m=\u001b[39m (\u001b[39mlist\u001b[39m(x\u001b[39m.\u001b[39mkeys()) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m data)\n\u001b[1;32m    916\u001b[0m     sort \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(d, \u001b[39mdict\u001b[39m) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m data)\n\u001b[0;32m--> 917\u001b[0m     pre_cols \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mfast_unique_multiple_list_gen(gen, sort\u001b[39m=\u001b[39msort)\n\u001b[1;32m    918\u001b[0m     columns \u001b[39m=\u001b[39m ensure_index(pre_cols)\n\u001b[1;32m    920\u001b[0m \u001b[39m# assure that they are of the base dict class and not of derived\u001b[39;00m\n\u001b[1;32m    921\u001b[0m \u001b[39m# classes\u001b[39;00m\n",
      "File \u001b[0;32mpandas/_libs/lib.pyx:367\u001b[0m, in \u001b[0;36mpandas._libs.lib.fast_unique_multiple_list_gen\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fin_proj/lib/python3.11/site-packages/pandas/core/internals/construction.py:915\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \u001b[39mConvert list of dicts to numpy arrays\u001b[39;00m\n\u001b[1;32m    897\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[39mcolumns : Index\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[39mif\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 915\u001b[0m     gen \u001b[39m=\u001b[39m (\u001b[39mlist\u001b[39m(x\u001b[39m.\u001b[39mkeys()) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m data)\n\u001b[1;32m    916\u001b[0m     sort \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(d, \u001b[39mdict\u001b[39m) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m data)\n\u001b[1;32m    917\u001b[0m     pre_cols \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mfast_unique_multiple_list_gen(gen, sort\u001b[39m=\u001b[39msort)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(rows)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now sort by English learning language only to sort only English sentances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"learning_language\"] == \"English\"]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>learning_language</th>\n",
       "      <th>native_language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>corrections</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>728457</td>\n",
       "      <td>216037</td>\n",
       "      <td>English</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>About winter</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>728457</td>\n",
       "      <td>216037</td>\n",
       "      <td>English</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>This is my second post.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  journal_id sentence_id learning_language native_language  \\\n",
       "0     728457      216037           English        Japanese   \n",
       "1     728457      216037           English        Japanese   \n",
       "\n",
       "                  sentence corrections  \n",
       "0             About winter          []  \n",
       "1  This is my second post.          []  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.columns\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sorts for only comma errors. It is commented out as it takes hours to run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[34], line 50\u001b[0m\n",
      "\u001b[1;32m     47\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(tgt, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m helpers\u001b[39m.\u001b[39mcomma_only_edit(src, tgt) \u001b[39mfor\u001b[39;00m tgt \u001b[39min\u001b[39;00m corrs)\n",
      "\u001b[0;32m---> 50\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mlabel_comma_involved\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(row_has_comma_involved_edit, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n",
      "\u001b[1;32m     51\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mlabel_comma_only\u001b[39m\u001b[39m\"\u001b[39m]     \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(row_has_comma_only_edit, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n",
      "\u001b[1;32m     53\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mComma-involved label distribution:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fin_proj/lib/python3.11/site-packages/pandas/core/frame.py:10401\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n",
      "\u001b[1;32m  10387\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n",
      "\u001b[1;32m  10389\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n",
      "\u001b[1;32m  10390\u001b[0m     \u001b[39mself\u001b[39m,\n",
      "\u001b[1;32m  10391\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m  10399\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n",
      "\u001b[1;32m  10400\u001b[0m )\n",
      "\u001b[0;32m> 10401\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fin_proj/lib/python3.11/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n",
      "\u001b[1;32m    914\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw(engine\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine, engine_kwargs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine_kwargs)\n",
      "\u001b[0;32m--> 916\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_standard()\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fin_proj/lib/python3.11/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m   1061\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "\u001b[1;32m   1062\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_series_generator()\n",
      "\u001b[1;32m   1064\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_series_numba()\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fin_proj/lib/python3.11/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m   1078\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n",
      "\u001b[1;32m   1079\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n",
      "\u001b[1;32m   1080\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n",
      "\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(v, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs)\n",
      "\u001b[1;32m   1082\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n",
      "\u001b[1;32m   1083\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n",
      "\u001b[1;32m   1084\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n",
      "\u001b[1;32m   1085\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\n",
      "Cell \u001b[0;32mIn[34], line 41\u001b[0m, in \u001b[0;36mrow_has_comma_involved_edit\u001b[0;34m(row)\u001b[0m\n",
      "\u001b[1;32m     39\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(corrs, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n",
      "\u001b[1;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;32m---> 41\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39many\u001b[39m(comma_involved(src, tgt) \u001b[39mfor\u001b[39;00m tgt \u001b[39min\u001b[39;00m corrs)\n",
      "\n",
      "Cell \u001b[0;32mIn[34], line 41\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[1;32m     39\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(corrs, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n",
      "\u001b[1;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;32m---> 41\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39many\u001b[39m(comma_involved(src, tgt) \u001b[39mfor\u001b[39;00m tgt \u001b[39min\u001b[39;00m corrs)\n",
      "\n",
      "Cell \u001b[0;32mIn[34], line 29\u001b[0m, in \u001b[0;36mcomma_involved\u001b[0;34m(src, tgt)\u001b[0m\n",
      "\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;32m     28\u001b[0m src_toks \u001b[39m=\u001b[39m helpers\u001b[39m.\u001b[39mtokenize(src)\n",
      "\u001b[0;32m---> 29\u001b[0m tgt_toks \u001b[39m=\u001b[39m helpers\u001b[39m.\u001b[39mtokenize(tgt)\n",
      "\u001b[1;32m     30\u001b[0m edits \u001b[39m=\u001b[39m helpers\u001b[39m.\u001b[39mget_edits(src_toks, tgt_toks)\n",
      "\u001b[1;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m edits:\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/LING_2270_proj/helpers.py:160\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(s)\u001b[0m\n",
      "\u001b[1;32m    158\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(s):\n",
      "\u001b[1;32m    159\u001b[0m     doc \u001b[39m=\u001b[39m nlp(s)\n",
      "\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m [t\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m doc \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m t\u001b[39m.\u001b[39mis_space]\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/LING_2270_proj/helpers.py:160\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[1;32m    158\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(s):\n",
      "\u001b[1;32m    159\u001b[0m     doc \u001b[39m=\u001b[39m nlp(s)\n",
      "\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m [t\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m doc \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m t\u001b[39m.\u001b[39mis_space]\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Take the first correction as our reference target\n",
    "# df[\"first_corr\"] = df[\"corrections\"].apply(\n",
    "#     lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None\n",
    "# )\n",
    "\n",
    "\n",
    "# # Label whether the only edits between sentence and correction are comma edits\n",
    "# df[\"comma_only_error\"] = df.apply(\n",
    "#     lambda row: comma_only_edit(row[\"sentence\"], row[\"first_corr\"]),\n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# df[[\"sentence\", \"first_corr\", \"comma_only_error\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only keep rows where we actually have a correction\n",
    "# df_clean = df[df[\"first_corr\"].notnull()].copy()\n",
    "\n",
    "# # Our label: 1 = pure comma error, 0 = not pure comma error\n",
    "# df_clean[\"label\"] = df_clean[\"comma_only_error\"].astype(int)\n",
    "\n",
    "# df_clean[[\"sentence\", \"first_corr\", \"label\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have the csv file, then start running the code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save to CSV\n",
    "# df_clean.to_csv(\"df_clean.csv\", index=False)\n",
    "df_clean = pd.read_csv('df_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    1163569\n",
       "1       3015\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean[\"label\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset size: 13015\n",
      "label\n",
      "0    10000\n",
      "1     3015\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_pos = df_clean[df_clean[\"label\"] == 1]\n",
    "df_neg = df_clean[df_clean[\"label\"] == 0].sample(n=10000, random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([df_pos, df_neg]).sample(frac=1, random_state=42)\n",
    "df_balanced = pd.concat([df_pos, df_neg]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Balanced dataset size:\", len(df_balanced))\n",
    "print(df_balanced[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will download the data off of kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/eddiecavallin/.cache/kagglehub/datasets/mikeortman/wikipedia-sentences/versions/3\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mikeortman/wikipedia-sentences\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, select random 200,000 lines to construct the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000,\n",
       " ['Brian C. McGing is a papyrologist and ancient historian, who specializes in the Hellenistic period.',\n",
       "  'Ford gained national attention when Miley Cyrus brought them as her date to The Foundation for AIDS Research (AMFAR) gala in 2015.',\n",
       "  'Its specific name \"limbatus\" is from the Latin meaning \"black-edged\" and refers to the colored markings of this species.',\n",
       "  'Skarbino is a village in Kardzhali Municipality, Kardzhali Province, southern Bulgaria.',\n",
       "  'Dasai Chowdhary is an Indian politician.',\n",
       "  'Berdusco also played internationally for Canada and scored one of its most memorable goals in a friendly against Brazil in 1994.',\n",
       "  'AppleDouble leaves the data fork in its original format, allowing it to be edited by normal Unix utilities.',\n",
       "  'Kronenbourg 1664 is now produced in the UK by Heineken after being bought from Scottish & Newcastle.',\n",
       "  \"In J. R. R. Tolkien's legendarium, the Battle of the Morannon or Battle of the Black Gate is a fictional event that took place at the end of the War of the Ring.\",\n",
       "  'The Antilopinae are a subfamily of Bovidae.'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_random_lines(path, k=200_000):\n",
    "    \"\"\"\n",
    "    Randomly sample k lines from a very large file\n",
    "    using reservoir sampling.\n",
    "    Ensures unbiased random sampling.\n",
    "    \"\"\"\n",
    "    reservoir = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            if len(reservoir) < k:\n",
    "                reservoir.append(line)\n",
    "            else:\n",
    "                # Replace elements with decreasing probability\n",
    "                j = random.randint(1, i)\n",
    "                if j <= k:\n",
    "                    reservoir[j - 1] = line\n",
    "\n",
    "    return reservoir\n",
    "\n",
    "\n",
    "wiki_path = \"wikisent2.txt\"\n",
    "\n",
    "random.seed(42)\n",
    "clean_sentences = sample_random_lines(wiki_path, k=200_000)\n",
    "\n",
    "len(clean_sentences), clean_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load in spacy, and then use our helper functions to build the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic examples per type:\n",
      "  comma splices: 3000\n",
      "  comma deletions: 3000\n",
      "  comma insertions: 3000\n",
      "Total rows in df_syn: 16505\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>error_type</th>\n",
       "      <th>synthetic_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Song is professor of law and political science...</td>\n",
       "      <td>0</td>\n",
       "      <td>orig</td>\n",
       "      <td>delete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He played 18 seasons and 346 matches in the, N...</td>\n",
       "      <td>1</td>\n",
       "      <td>comma_inserted</td>\n",
       "      <td>insert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Behind the scenes Imbruglia quit the serial.</td>\n",
       "      <td>0</td>\n",
       "      <td>orig</td>\n",
       "      <td>insert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The club currently has many teams within the o...</td>\n",
       "      <td>1</td>\n",
       "      <td>comma_deleted</td>\n",
       "      <td>delete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>On June 29 1995, the drinking water supply in ...</td>\n",
       "      <td>1</td>\n",
       "      <td>comma_deleted</td>\n",
       "      <td>delete</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label      error_type  \\\n",
       "0  Song is professor of law and political science...      0            orig   \n",
       "1  He played 18 seasons and 346 matches in the, N...      1  comma_inserted   \n",
       "2       Behind the scenes Imbruglia quit the serial.      0            orig   \n",
       "3  The club currently has many teams within the o...      1   comma_deleted   \n",
       "4  On June 29 1995, the drinking water supply in ...      1   comma_deleted   \n",
       "\n",
       "  synthetic_source  \n",
       "0           delete  \n",
       "1           insert  \n",
       "2           insert  \n",
       "3           delete  \n",
       "4           delete  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "df_synthetic = build_synthetic_comma_dataset(clean_sentences, max_per_type=3000)\n",
    "df_synthetic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rows \u001b[39m=\u001b[39m build_synthetic_esl_like(clean_sentences, n_per_type\u001b[39m=\u001b[39m\u001b[39m3000\u001b[39m, seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m      2\u001b[0m df_syn_esl \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(rows)\u001b[39m.\u001b[39mdrop_duplicates(subset\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39merror_type\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m df_syn_esl[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalue_counts(), df_syn_esl[\u001b[39m\"\u001b[39m\u001b[39merror_type\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalue_counts()\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/Documents/GitHub/LING_2270_proj/helpers.py:446\u001b[0m, in \u001b[0;36mbuild_synthetic_esl_like\u001b[0;34m(sentences, n_per_type, seed)\u001b[0m\n\u001b[1;32m    444\u001b[0m tries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    445\u001b[0m s \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoice(sentences)\n\u001b[0;32m--> 446\u001b[0m s_bad \u001b[39m=\u001b[39m fn(s)\n\u001b[1;32m    447\u001b[0m \u001b[39mif\u001b[39;00m s_bad \u001b[39mand\u001b[39;00m s_bad \u001b[39m!=\u001b[39m s:\n\u001b[1;32m    448\u001b[0m     rows\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    449\u001b[0m         {\n\u001b[1;32m    450\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m\"\u001b[39m: _safe_period(s),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    454\u001b[0m         }\n\u001b[1;32m    455\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/GitHub/LING_2270_proj/helpers.py:381\u001b[0m, in \u001b[0;36mdelete_comma_before_conj\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    380\u001b[0m doc \u001b[39m=\u001b[39m nlp(s)\n\u001b[0;32m--> 381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(extract_clause_heads(doc)) \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    382\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[39m# remove the comma at m.start()\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/LING_2270_proj/helpers.py:218\u001b[0m, in \u001b[0;36mextract_clause_heads\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39mfor\u001b[39;00m tok \u001b[39min\u001b[39;00m doc:\n\u001b[1;32m    217\u001b[0m     \u001b[39mif\u001b[39;00m tok\u001b[39m.\u001b[39mpos_ \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mVERB\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mAUX\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 218\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_finite_verb(tok):\n\u001b[1;32m    220\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rows = build_synthetic_esl_like(clean_sentences, n_per_type=3000, seed=42)\n",
    "df_syn_esl = pd.DataFrame(rows).drop_duplicates(subset=[\"sentence\",\"label\",\"error_type\"]).reset_index(drop=True)\n",
    "\n",
    "df_syn_esl[\"label\"].value_counts(), df_syn_esl[\"error_type\"].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have the built csv file, then just run this code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_synthetic.to_csv(\"df_synthetic.csv\")\n",
    "df_syn_esl.to_csv(\"df_syn_esl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    9000\n",
       "0    7505\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_synthetic.head()\n",
    "df_synthetic[\"label\"] = df_synthetic[\"label\"].astype(int)\n",
    "df_synthetic[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort into train, test, and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 15145\n",
      "Val size: 3246\n",
      "Test size: 3246\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>It has become a popular girls' name and was th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>Wildboyz is an American spin-off television se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9148</th>\n",
       "      <td>However, they have some functionality, as wate...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20397</th>\n",
       "      <td>Wanbi is a settlement in South Australia.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>The success of The Grapes of Wrath allowed Smi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  label\n",
       "376    It has become a popular girls' name and was th...      0\n",
       "2655   Wildboyz is an American spin-off television se...      1\n",
       "9148   However, they have some functionality, as wate...      0\n",
       "20397          Wanbi is a settlement in South Australia.      0\n",
       "1533   The success of The Grapes of Wrath allowed Smi...      1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_synthetic = pd.read_csv('df_synthetic.csv')\n",
    "df_synthetic = pd.read_csv('df_syn_esl.csv')\n",
    "# df_synthetic must have columns: 'sentence' (str), 'label' (0/1)\n",
    "df_synthetic[\"label\"] = df_synthetic[\"label\"].astype(int)\n",
    "\n",
    "X = df_synthetic[\"sentence\"]\n",
    "y = df_synthetic[\"label\"]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "train_df = pd.DataFrame({\"sentence\": X_train, \"label\": y_train})\n",
    "val_df   = pd.DataFrame({\"sentence\": X_val,   \"label\": y_val})\n",
    "test_df  = pd.DataFrame({\"sentence\": X_test,  \"label\": y_test})\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Val size:\", len(val_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFG code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk import CFG, ChartParser\n",
    "import helpers\n",
    "\n",
    "# spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Make sure helpers uses this nlp object\n",
    "helpers.nlp = nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the grammar for the cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_str = r\"\"\"\n",
    "S -> MAIN_CLAUSE PUNCT\n",
    "S -> MAIN_CLAUSE CONJ MAIN_CLAUSE PUNCT\n",
    "S -> SUB_CLAUSE COMMA MAIN_CLAUSE PUNCT\n",
    "S -> MAIN_CLAUSE COMMA SUB_CLAUSE PUNCT\n",
    "\n",
    "# Main clauses\n",
    "MAIN_CLAUSE -> NP VP\n",
    "MAIN_CLAUSE -> VP \n",
    "MAIN_CLAUSE -> NP \n",
    "\n",
    "# Subordinate clauses: SCONJ + clause\n",
    "SUB_CLAUSE -> SCONJ MAIN_CLAUSE\n",
    "SUB_CLAUSE -> SCONJ NP VP\n",
    "\n",
    "# Noun phrases\n",
    "NP -> PRON\n",
    "NP -> DET NBAR\n",
    "NP -> NBAR\n",
    "NP -> NP PP \n",
    "NP -> NP COMMA NP  \n",
    "\n",
    "# N-bar (nominal core)\n",
    "NBAR -> N\n",
    "NBAR -> ADJ NBAR \n",
    "NBAR -> N NBAR\n",
    "NBAR -> N PP \n",
    "\n",
    "# Verb phrases\n",
    "VP -> V\n",
    "VP -> V NP\n",
    "VP -> V PP\n",
    "VP -> V NP PP\n",
    "VP -> V ADV\n",
    "VP -> V NP ADV\n",
    "VP -> V PP ADV\n",
    "VP -> V NP PP ADV\n",
    "\n",
    "VP -> AUX VP \n",
    "VP -> ADV VP  \n",
    "VP -> VP ADV \n",
    "VP -> VP PP  \n",
    "# Prepositional phrase\n",
    "PP -> P NP\n",
    "\n",
    "# POS tag terminals (coarse tags from your helpers.coarse_pos)\n",
    "PRON -> 'PRON'\n",
    "DET  -> 'DET'\n",
    "N    -> 'N'\n",
    "V    -> 'V'\n",
    "AUX  -> 'AUX'\n",
    "P    -> 'P'\n",
    "CONJ -> 'CONJ'\n",
    "SCONJ -> 'SCONJ'\n",
    "PUNCT -> 'PUNCT'\n",
    "COMMA -> 'COMMA'\n",
    "ADV -> 'ADV'\n",
    "ADJ -> 'ADJ'\n",
    "\"\"\"\n",
    "\n",
    "grammar = CFG.fromstring(grammar_str)\n",
    "s_parser = ChartParser(grammar)\n",
    "\n",
    "# For clause-level parsing, we treat MAIN_CLAUSE as the start symbol\n",
    "clause_nt = nltk.Nonterminal('MAIN_CLAUSE')\n",
    "clause_grammar = CFG(clause_nt, grammar.productions())\n",
    "clause_parser = ChartParser(clause_grammar)\n",
    "\n",
    "# Plug into helpers so is_cfg_clause / is_cfg_comma_splice can use these\n",
    "helpers.s_parser = s_parser\n",
    "helpers.clause_parser = clause_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test show how the current grammar is partially insufficient for the current comma detection task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went home, I slept. => True\n",
      "I went home, and I slept. => False\n",
      "I went home and I slept. => False\n"
     ]
    }
   ],
   "source": [
    "for s in [\n",
    "    \"I went home, I slept.\",\n",
    "    \"I went home, and I slept.\",\n",
    "    \"I went home and I slept.\"\n",
    "]:\n",
    "    print(s, \"=>\", helpers.is_cfg_comma_splice(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use out cfg rules to predict on the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG baseline on synthetic test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4581    0.9991    0.6281      1126\n",
      "           1     0.9500    0.0141    0.0277      1350\n",
      "\n",
      "    accuracy                         0.4620      2476\n",
      "   macro avg     0.7040    0.5066    0.3279      2476\n",
      "weighted avg     0.7263    0.4620    0.3008      2476\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1125    1]\n",
      " [1331   19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def cfg_predict_label(sentence: str) -> int:\n",
    "    try:\n",
    "        return 1 if helpers.is_cfg_comma_splice(sentence) else 0\n",
    "    except Exception:\n",
    "        return 0\n",
    "cfg_true = test_df[\"label\"].to_numpy()\n",
    "cfg_pred = test_df[\"sentence\"].apply(cfg_predict_label).to_numpy()\n",
    "\n",
    "print(\"CFG baseline on synthetic test set\")\n",
    "print(classification_report(cfg_true, cfg_pred, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(cfg_true, cfg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically just guessing at this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG baseline on Lang-8 test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4889    1.0000    0.6567      1587\n",
      "           1     0.0000    0.0000    0.0000      1659\n",
      "\n",
      "    accuracy                         0.4889      3246\n",
      "   macro avg     0.2445    0.5000    0.3284      3246\n",
      "weighted avg     0.2390    0.4889    0.3211      3246\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1587    0]\n",
      " [1659    0]]\n"
     ]
    }
   ],
   "source": [
    "def cfg_predict_label(sentence: str) -> int:\n",
    "    try:\n",
    "        return 1 if helpers.is_cfg_comma_splice(sentence) else 0\n",
    "    except Exception:\n",
    "        return 0  # fail closed\n",
    "\n",
    "cfg_true = test_df[\"label\"].to_numpy()\n",
    "cfg_pred = test_df[\"sentence\"].apply(cfg_predict_label).to_numpy()\n",
    "\n",
    "print(\"CFG baseline on Lang-8 test set\")\n",
    "print(classification_report(cfg_true, cfg_pred, digits=4, zero_division=0))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(cfg_true, cfg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 15145\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 3246\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 3246\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "# Wrap into HF Datasets\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_ds   = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "test_ds  = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"validation\": val_ds,\n",
    "    \"test\": test_ds,\n",
    "})\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we tokenize the datasets in order to preprocess the inputs to the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/fin_proj/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c91b8699ae437787251cd3769ac0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06122416bf2a4dd8b4767ba1bb417cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3246 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb217197861487ba3595252a7b21401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3246 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 15145\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 3246\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 3246\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DistilBERT tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_batch, batched=True)\n",
    "\n",
    "# Keep only what we need\n",
    "cols_to_keep = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    [c for c in tokenized_datasets[\"train\"].column_names if c not in cols_to_keep]\n",
    ")\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    ")\n",
    "\n",
    "# Device (M1/M2  'mps', else CPU/CUDA)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "val_dataset   = tokenized_datasets[\"validation\"]\n",
    "test_dataset  = tokenized_datasets[\"test\"]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/fin_proj/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Optimizer & scheduler\n",
    "epochs = 3\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "num_training_steps = epochs * len(train_loader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wil train the model over 3 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5081ef3e10914143a034e48af6739597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3:   0%|          | 0/947 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 avg training loss: 0.3994\n",
      "Epoch 1 val loss: 0.2710, val acc: 0.8706\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e64efbf39b04a6e8c84e58b9ab81002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/3:   0%|          | 0/947 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 avg training loss: 0.2292\n",
      "Epoch 2 val loss: 0.2741, val acc: 0.8749\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8432c720dd41d6ad53a04caab6bcba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/3:   0%|          | 0/947 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 avg training loss: 0.1634\n",
      "Epoch 3 val loss: 0.3081, val acc: 0.8715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in loop:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"\\nEpoch {epoch+1} avg training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1} val loss: {avg_val_loss:.4f}, val acc: {val_acc:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we evaluate on the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic test set performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8870    0.8702    0.8785      1587\n",
      "           1     0.8780    0.8939    0.8859      1659\n",
      "\n",
      "    accuracy                         0.8823      3246\n",
      "   macro avg     0.8825    0.8821    0.8822      3246\n",
      "weighted avg     0.8824    0.8823    0.8823      3246\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1381  206]\n",
      " [ 176 1483]]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "\n",
    "print(\"Synthetic test set performance:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will see which types of comma errors it is best at predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_idx</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>true</th>\n",
       "      <th>pred</th>\n",
       "      <th>error_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10085</td>\n",
       "      <td>On 28 May 2014 Louis Bontes, Joram van Klavere...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>orig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17606</td>\n",
       "      <td>Community events continue to be held in the bu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>orig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6907</td>\n",
       "      <td>For the first time ever, the national prelimin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>orig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8686</td>\n",
       "      <td>In cultures where it is not normal it may be c...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>missing_intro_comma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4752</td>\n",
       "      <td>A larger ion that has two onium ion subgroups ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>orig</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   orig_idx                                           sentence  label  true  \\\n",
       "0     10085  On 28 May 2014 Louis Bontes, Joram van Klavere...      0     0   \n",
       "1     17606  Community events continue to be held in the bu...      0     0   \n",
       "2      6907  For the first time ever, the national prelimin...      0     0   \n",
       "3      8686  In cultures where it is not normal it may be c...      1     1   \n",
       "4      4752  A larger ion that has two onium ion subgroups ...      0     0   \n",
       "\n",
       "   pred           error_type  \n",
       "0     1                 orig  \n",
       "1     0                 orig  \n",
       "2     0                 orig  \n",
       "3     1  missing_intro_comma  \n",
       "4     0                 orig  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# test_df still has the original index from df_synthetic\n",
    "# so we reset it but store that original index as 'orig_idx'\n",
    "results_df = test_df.copy()\n",
    "results_df = results_df.reset_index().rename(columns={\"index\": \"orig_idx\"})\n",
    "\n",
    "# Attach true + predicted labels from your DistilBERT evaluation\n",
    "results_df[\"true\"] = all_labels\n",
    "results_df[\"pred\"] = all_preds\n",
    "\n",
    "# Map error_type from df_synthetic using the original indices\n",
    "results_df[\"error_type\"] = df_synthetic.loc[results_df[\"orig_idx\"], \"error_type\"].values\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by synthetic error_type:\n",
      "\n",
      "comma_splice     accuracy = 0.9429   (n=420)\n",
      "missing_comma_before_conj  accuracy = 0.7018   (n=399)\n",
      "missing_intro_comma  accuracy = 0.9298   (n=413)\n",
      "orig             accuracy = 0.8702   (n=1587)\n",
      "unnecessary_subj_verb_comma  accuracy = 0.9906   (n=427)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy by synthetic error_type:\\n\")\n",
    "for etype, group in results_df.groupby(\"error_type\"):\n",
    "    acc = (group[\"pred\"] == group[\"true\"]).mean()\n",
    "    n = len(group)\n",
    "    print(f\"{etype:15s}  accuracy = {acc:.4f}   (n={n})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on ESL data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test on the ESL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 9110\n",
      "Validation size: 1952\n",
      "Test size: 1953\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_balanced[\"sentence\"]\n",
    "y = df_balanced[\"label\"]\n",
    "\n",
    "# Train / temp split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Validation / Test split\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Validation size:\", len(X_val))\n",
    "print(\"Test size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lang-8 size after cleaning: 1166583\n",
      "                                            sentence  label\n",
      "0  I will appreciate it if you correct my sentences.      0\n",
      "1  It's been getting colder these days here in Ja...      0\n",
      "2  The summer weather in Japan is not agreeable t...      0\n",
      "3  So, as the winter is coming, I'm getting to fe...      0\n",
      "4                    It is the very exciting season.      0\n"
     ]
    }
   ],
   "source": [
    "# Start from df_clean (your Lang-8 data)\n",
    "df_clean[\"label\"] = df_clean[\"label\"].astype(int)\n",
    "\n",
    "# Keep only the columns we need\n",
    "lang8_df = df_clean[[\"sentence\", \"label\"]].copy()\n",
    "\n",
    "# Drop rows where sentence is missing\n",
    "lang8_df = lang8_df.dropna(subset=[\"sentence\"])\n",
    "\n",
    "# Force everything to string (tokenizer wants strings)\n",
    "lang8_df[\"sentence\"] = lang8_df[\"sentence\"].astype(str)\n",
    "\n",
    "print(\"Lang-8 size after cleaning:\", len(lang8_df))\n",
    "print(lang8_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using subset for evaluation: 50000\n"
     ]
    }
   ],
   "source": [
    "# Optional: subsample for evaluation, e.g. 50k sentences\n",
    "lang8_df = lang8_df.sample(n=50_000, random_state=42)\n",
    "print(\"Using subset for evaluation:\", len(lang8_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1bac3bfef8d467a9c0952feca884cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "lang8_ds = Dataset.from_pandas(lang8_df.reset_index(drop=True))\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "lang8_tokenized = lang8_ds.map(tokenize_batch, batched=True)\n",
    "\n",
    "cols_to_keep = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "lang8_tokenized = lang8_tokenized.remove_columns(\n",
    "    [c for c in lang8_tokenized.column_names if c not in cols_to_keep]\n",
    ")\n",
    "lang8_tokenized.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lang8_tokenized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m classification_report, confusion_matrix\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m lang8_loader \u001b[39m=\u001b[39m DataLoader(lang8_tokenized, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m)\n\u001b[1;32m      7\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      8\u001b[0m all_labels \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lang8_tokenized' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "lang8_loader = DataLoader(lang8_tokenized, batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in lang8_loader:\n",
    "        ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        logits = model(ids, attention_mask=mask).logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "\n",
    "print(\"Lang-8 evaluation (synthetic-trained model):\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fin_proj)",
   "language": "python",
   "name": "fin_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
