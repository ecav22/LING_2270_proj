{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import ast\n",
    "import helpers\n",
    "from helpers import *\n",
    "import nltk\n",
    "from nltk import CFG, ChartParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = load_dataset(\"jhu-clsp/jfleg\")\n",
    "# ds.save_to_disk(\"JFLEG\")\n",
    "# ds[\"test\"].to_csv(\"test.csv\")\n",
    "# ds[\"validation\"].to_csv(\"validation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "val_df  = pd.read_csv(\"validation.csv\")\n",
    "\n",
    "\n",
    "test_df[\"corr_list\"] = test_df[\"corrections\"].apply(parse_corrections)\n",
    "val_df[\"corr_list\"]  = val_df[\"corrections\"].apply(parse_corrections)\n",
    "\n",
    "test_df[\"comma_candidate\"] = test_df.apply(\n",
    "    lambda row: comma_change_row(row[\"sentence\"], row[\"corr_list\"]), axis=1\n",
    ")\n",
    "val_df[\"comma_candidate\"] = val_df.apply(\n",
    "    lambda row: comma_change_row(row[\"sentence\"], row[\"corr_list\"]), axis=1\n",
    ")\n",
    "\n",
    "test_comma = test_df[test_df[\"comma_candidate\"]]\n",
    "val_comma  = val_df[val_df[\"comma_candidate\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Gutenberg Data\n",
    "\n",
    "Here I will construct the Gutenberg dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not define our own grammar, as this would be quite complicated. Instead we decide to import a pretrained parser to do parts of speech tagging. We decided on using the spaCy NLP package. As a group, we are aware that spaCy has the capability of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Make sure you've done: python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grammar_str = r\"\"\"\n",
    "S -> CLAUSE PUNCT\n",
    "S -> CLAUSE CONJ CLAUSE PUNCT\n",
    "\n",
    "CLAUSE -> NP VP\n",
    "\n",
    "NP -> PRON\n",
    "NP -> DET N\n",
    "NP -> N\n",
    "\n",
    "VP -> V\n",
    "VP -> V NP\n",
    "VP -> V ADV\n",
    "VP -> V NP ADV\n",
    "VP -> AUX V\n",
    "VP -> AUX V NP\n",
    "VP -> AUX V ADV\n",
    "VP -> AUX V NP ADV\n",
    "VP -> V NP PP\n",
    "VP -> AUX V NP PP\n",
    "\n",
    "PP -> P NP\n",
    "\n",
    "PRON -> 'PRON'\n",
    "DET  -> 'DET'\n",
    "N    -> 'N'\n",
    "V    -> 'V'\n",
    "AUX  -> 'AUX'\n",
    "P    -> 'P'\n",
    "CONJ -> 'CONJ'\n",
    "PUNCT -> 'PUNCT'\n",
    "COMMA -> 'COMMA'\n",
    "ADV -> 'ADV'\n",
    "\"\"\"\n",
    "\n",
    "grammar = CFG.fromstring(grammar_str)\n",
    "s_parser = ChartParser(grammar)\n",
    "\n",
    "clause_nt = nltk.Nonterminal('CLAUSE')\n",
    "clause_grammar = CFG(clause_nt, grammar.productions())\n",
    "clause_parser = ChartParser(clause_grammar)\n",
    "\n",
    "helpers.s_parser = s_parser\n",
    "helpers.clause_parser = clause_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went home, I slept. => True\n",
      "I went home, and I slept. => False\n",
      "I went home and I slept. => False\n",
      "Every person needs to know a bit about math, so they can manage daily life. => False\n",
      "Every person needs to know a bit about math. => False\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    \"I went home, I slept.\",\n",
    "    \"I went home, and I slept.\",\n",
    "    \"I went home and I slept.\",\n",
    "    \"Every person needs to know a bit about math, so they can manage daily life.\",\n",
    "    \"Every person needs to know a bit about math.\",\n",
    "]\n",
    "\n",
    "for s in examples:\n",
    "    print(s, \"=>\", is_cfg_comma_splice(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "records = []\n",
    "with open(\"lang-8_data.dat\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "            records.append(obj)\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for rec in records:\n",
    "    journal_id = rec[0]\n",
    "    sentence_id = rec[1]\n",
    "    learning_language = rec[2]\n",
    "    native_language = rec[3]\n",
    "    learner_sents = rec[4]\n",
    "    corrections = rec[5]\n",
    "\n",
    "    for sent, corr_list in zip(learner_sents, corrections):\n",
    "        rows.append({\n",
    "            \"journal_id\": journal_id,\n",
    "            \"sentence_id\": sentence_id,\n",
    "            \"learning_language\": learning_language,\n",
    "            \"native_language\": native_language,\n",
    "            \"sentence\": sent,\n",
    "            \"corrections\": corr_list\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>learning_language</th>\n",
       "      <th>native_language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>corrections</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1057227</td>\n",
       "      <td>290610</td>\n",
       "      <td>Korean</td>\n",
       "      <td>English</td>\n",
       "      <td>오늘 배운 새 표현 / New expressions I learned today</td>\n",
       "      <td>[오늘 배운 새[f-blue]로운[/f-blue] 표현[f-blue]들[/f-blu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1057227</td>\n",
       "      <td>290610</td>\n",
       "      <td>Korean</td>\n",
       "      <td>English</td>\n",
       "      <td>TTMIK가 제자 자주 쓰는 한국교재이에요.</td>\n",
       "      <td>[TTMIK가 제자 자주 쓰는 한국교재[sline]이[/sline]에요., TTMI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1057227</td>\n",
       "      <td>290610</td>\n",
       "      <td>Korean</td>\n",
       "      <td>English</td>\n",
       "      <td>오늘은 새로 레슨 나와서 그 레슨에게서 새로 표현이 배웠어요.</td>\n",
       "      <td>[오늘은 새로 레슨 나와서 그 레슨에게서 새로 표현[f-red]을[/f-red] 배...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1057227</td>\n",
       "      <td>290610</td>\n",
       "      <td>Korean</td>\n",
       "      <td>English</td>\n",
       "      <td>밑에 그 표현들 붙혔어요.</td>\n",
       "      <td>[밑에 그 표현들 붙[f-red]였[/f-red]어요., 밑에 그 표현들[f-blu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1057227</td>\n",
       "      <td>290610</td>\n",
       "      <td>Korean</td>\n",
       "      <td>English</td>\n",
       "      <td>TTMIK is a Korean learning resource that I use...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  journal_id sentence_id learning_language native_language  \\\n",
       "0    1057227      290610            Korean         English   \n",
       "1    1057227      290610            Korean         English   \n",
       "2    1057227      290610            Korean         English   \n",
       "3    1057227      290610            Korean         English   \n",
       "4    1057227      290610            Korean         English   \n",
       "\n",
       "                                            sentence  \\\n",
       "0       오늘 배운 새 표현 / New expressions I learned today   \n",
       "1                           TTMIK가 제자 자주 쓰는 한국교재이에요.   \n",
       "2                 오늘은 새로 레슨 나와서 그 레슨에게서 새로 표현이 배웠어요.   \n",
       "3                                     밑에 그 표현들 붙혔어요.   \n",
       "4  TTMIK is a Korean learning resource that I use...   \n",
       "\n",
       "                                         corrections  \n",
       "0  [오늘 배운 새[f-blue]로운[/f-blue] 표현[f-blue]들[/f-blu...  \n",
       "1  [TTMIK가 제자 자주 쓰는 한국교재[sline]이[/sline]에요., TTMI...  \n",
       "2  [오늘은 새로 레슨 나와서 그 레슨에게서 새로 표현[f-red]을[/f-red] 배...  \n",
       "3  [밑에 그 표현들 붙[f-red]였[/f-red]어요., 밑에 그 표현들[f-blu...  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(rows)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first correction as our reference target\n",
    "df[\"first_corr\"] = df[\"corrections\"].apply(\n",
    "    lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None\n",
    ")\n",
    "\n",
    "from helpers import comma_only_edit\n",
    "\n",
    "# Label whether the only edits between sentence and correction are comma edits\n",
    "df[\"comma_only_error\"] = df.apply(\n",
    "    lambda row: comma_only_edit(row[\"sentence\"], row[\"first_corr\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df[[\"sentence\", \"first_corr\", \"comma_only_error\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"learning_language\"] == \"English\"]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep rows where we actually have a correction\n",
    "df_clean = df[df[\"first_corr\"].notnull()].copy()\n",
    "\n",
    "# Our label: 1 = pure comma error, 0 = not pure comma error\n",
    "df_clean[\"label\"] = df_clean[\"comma_only_error\"].astype(int)\n",
    "\n",
    "df_clean[[\"sentence\", \"first_corr\", \"label\"]].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fin_proj)",
   "language": "python",
   "name": "fin_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
